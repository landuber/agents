{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable v2 behavior\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n",
      "time_step_spec.observation BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation', environment.time_step_spec().observation)\n",
    "print('time_step_spec.reward', environment.time_step_spec().reward)\n",
    "print('time_step_spec.discount', environment.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', environment.time_step_spec().step_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.04454876,  0.04927505,  0.04358195, -0.00200671], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "time_step = environment.reset()\n",
    "print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.04356326,  0.24374576,  0.04354182, -0.28062674], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.03868835,  0.4382204 ,  0.03792928, -0.559265  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.02992394,  0.63279   ,  0.02674398, -0.83976096], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.01726814,  0.8275368 ,  0.00994876, -1.123915  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-7.1739918e-04,  1.0225270e+00, -1.2529540e-02, -1.4134607e+00],\n",
      "      dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01973314,  1.2178019 , -0.04079875, -1.7100338 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04408918,  1.4133682 , -0.07499943, -2.0151305 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.07235654,  1.6091844 , -0.11530204, -2.3300583 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.10454023,  1.8051462 , -0.1619032 , -2.655873  ], dtype=float32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.14064315,  2.0010679 , -0.21502067, -2.9933043 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(6,), dtype=dtype('float32'), name=None, minimum=[-1. -1. -1. -1. -1. -1.], maximum=[1. 1. 1. 1. 1. 1.])\n",
      "time_step_spec.observation BoundedArraySpec(shape=(17,), dtype=dtype('float32'), name=None, minimum=[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38], maximum=[3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation', env.time_step_spec().observation)\n",
    "print('time_step_spec.reward', env.time_step_spec().reward)\n",
    "print('time_step_spec.discount', env.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', env.time_step_spec().step_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(17,), dtype=tf.float32, name=None, minimum=array([-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38], dtype=float32), maximum=array([3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38], dtype=float32)))\n",
      "Action Specs: BoundedTensorSpec(shape=(6,), dtype=tf.float32, name=None, minimum=array([-1., -1., -1., -1., -1., -1.], dtype=float32), maximum=array([1., 1., 1., 1., 1., 1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-9.1067500e-02, -6.6572793e-02, -4.7095217e-02,  6.9585061e-05,\n",
      "         4.6733588e-02, -2.0074748e-02, -3.2299198e-02, -1.6512033e-03,\n",
      "         8.6873144e-02,  1.4975145e-01, -1.0232244e-01,  9.0327315e-02,\n",
      "        -2.3088583e-01,  1.4822657e-01,  2.7659308e-02, -3.0178087e-02,\n",
      "         1.9371119e-01]], dtype=float32)), array([[ 0.15088725,  0.14196634,  0.9563999 , -0.706512  , -0.9811747 ,\n",
      "        -0.30316854]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([-0.18825586], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.06946053,  0.01491265, -0.09753682,  0.03920974,  0.22235511,\n",
      "        -0.21333966, -0.25523356, -0.03001392,  0.09362458,  0.3178691 ,\n",
      "         2.037819  ,  0.07891111, -0.48062405,  7.041334  , -5.7361326 ,\n",
      "        -6.0308275 , -1.15453   ]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([-0.18825586], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.06946053,  0.01491265, -0.09753682,  0.03920974,  0.22235511,\n",
      "        -0.21333966, -0.25523356, -0.03001392,  0.09362458,  0.3178691 ,\n",
      "         2.037819  ,  0.07891111, -0.48062405,  7.041334  , -5.7361326 ,\n",
      "        -6.0308275 , -1.15453   ]], dtype=float32)), array([[-0.14627552,  0.8115866 ,  0.68825173, -0.28245163, -0.31778455,\n",
      "         0.12398553]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.1377712], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.08018537,  0.05515653, -0.09212859,  0.31302798,  0.42751178,\n",
      "        -0.3869524 , -0.35476658, -0.03968809,  0.39871377, -0.44545272,\n",
      "         0.23439011,  1.0133072 ,  6.833366  ,  1.8003051 , -1.4939672 ,\n",
      "         0.21205613,  0.31282035]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([0.1377712], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.08018537,  0.05515653, -0.09212859,  0.31302798,  0.42751178,\n",
      "        -0.3869524 , -0.35476658, -0.03968809,  0.39871377, -0.44545272,\n",
      "         0.23439011,  1.0133072 ,  6.833366  ,  1.8003051 , -1.4939672 ,\n",
      "         0.21205613,  0.31282035]], dtype=float32)), array([[ 0.5925195 ,  0.7607422 ,  0.4328003 , -0.2324984 ,  0.14205194,\n",
      "        -0.8018024 ]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.6607775], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.10609243,  0.04156915,  0.26020345,  0.4645736 ,  0.29113588,\n",
      "        -0.31258616, -0.09486346, -0.2720176 ,  0.9664455 , -0.66415644,\n",
      "        -0.4006235 ,  8.47155   ,  0.93388075, -3.5321455 ,  2.619891  ,\n",
      "         6.7422643 , -6.2607584 ]], dtype=float32))]\n",
      "Total reward: [0.61029285]\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "random_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                               time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "for i in range(num_steps):\n",
    "    action_step = random_policy.action(time_step)\n",
    "    action = action_step.action\n",
    "    next_time_step = tf_env.step(action)\n",
    "    transitions.append([time_step, action, next_time_step])\n",
    "    reward += next_time_step.reward\n",
    "    time_step = next_time_step\n",
    "    \n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9469503]\n",
      " [0.6618465]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(0.9469503, shape=(), dtype=float32)\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "nested_array = tf.nest.flatten(tf.random.uniform(shape=[10, 299, 299, 3]).numpy());\n",
    "array = tf.nest.flatten(nested_array)\n",
    "len(array)\n",
    "arr = tf.random.uniform(shape=[2,1])\n",
    "print(arr)\n",
    "print(tf.math.reduce_max(arr))\n",
    "print(tf.math.argmax(arr, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Observation tf.Tensor(\n",
      "[[-9.1067500e-02 -6.6572793e-02 -4.7095217e-02  6.9585061e-05\n",
      "   4.6733588e-02 -2.0074748e-02 -3.2299198e-02 -1.6512033e-03\n",
      "   8.6873144e-02  1.4975145e-01 -1.0232244e-01  9.0327315e-02\n",
      "  -2.3088583e-01  1.4822657e-01  2.7659308e-02 -3.0178087e-02\n",
      "   1.9371119e-01]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.15088725  0.14196634  0.9563999  -0.706512   -0.9811747  -0.30316854]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[-0.06946053  0.01491265 -0.09753682  0.03920974  0.22235511 -0.21333966\n",
      "  -0.25523356 -0.03001392  0.09362458  0.3178691   2.037819    0.07891111\n",
      "  -0.48062405  7.041334   -5.7361326  -6.0308275  -1.15453   ]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[-0.14627552  0.8115866   0.68825173 -0.28245163 -0.31778455  0.12398553]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[-0.08018537  0.05515653 -0.09212859  0.31302798  0.42751178 -0.3869524\n",
      "  -0.35476658 -0.03968809  0.39871377 -0.44545272  0.23439011  1.0133072\n",
      "   6.833366    1.8003051  -1.4939672   0.21205613  0.31282035]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.5925195   0.7607422   0.4328003  -0.2324984   0.14205194 -0.8018024 ]], shape=(1, 6), dtype=float32)\n",
      "[array([7.62691], dtype=float32), array([14.942488], dtype=float32), array([15.762382], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input_size = tf_env.action_spec().shape[0] + tf_env.time_step_spec().observation.shape[0]\n",
    "print(input_size)\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_history = []\n",
    "\n",
    "for time_step, action, next_time_step in transitions:\n",
    "    state_diff = next_time_step.observation - time_step.observation\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        print('Observation', time_step.observation)\n",
    "        print('Action', action)\n",
    "        pred_state_diff = model(tf.concat([time_step.observation, action], 1), training=True)   \n",
    "        loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "    \n",
    "    loss_history.append(loss_value.numpy())\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "\n",
    "print(loss_history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "\n",
    "class ModelBasedPolicy(tf_policy.Base):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 time_step_spec, \n",
    "                 action_spec, \n",
    "                 dynamics_model_network,\n",
    "                 num_random_action_selection=4096,\n",
    "                 horizon=15):\n",
    "        \n",
    "        if time_step_spec is None:\n",
    "            time_step_spec = ts.time_step_spec()\n",
    "        self._dynamics_model_network = dynamics_model_network\n",
    "        self._num_random_action_selection=num_random_action_selection\n",
    "        self._horizon = horizon\n",
    "        \n",
    "        super(ModelBasedPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec, action_spec=action_spec)\n",
    "        \n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        \n",
    "        random_actions = tensor_spec.sample_spec_nest(\n",
    "            self._action_spec, seed, outer_dims=[self._num_random_action_selection, self._horizon])\n",
    "        start_state = tf.tile(time_step.observation, [self._num_random_action_selection, 1])\n",
    "        cost = tf.zeros(shape=[self._num_random_action_selection, 1])\n",
    "        curr_state = start_state\n",
    "        for h in range(self._horizon):\n",
    "            next_state = curr_state + self._dynamics_model_network(tf.concat([curr_state, random_actions[:, h, :]], 1))\n",
    "            cost += self._cost_fn(curr_state, tf.constant(random_actions[:, h, :]), next_state)\n",
    "            curr_state = next_state\n",
    "           \n",
    "        min_cost_index = tf.math.argmin(cost, axis=1)[0]\n",
    "        return policy_step.PolicyStep(random_actions[min_cost_index, 0, :], policy_state)\n",
    "    \n",
    "    def _cost_fn(self, states, actions, next_states):\n",
    "        is_tf = tf.is_tensor(states)\n",
    "        is_single_state = (len(states.get_shape()) == 1) if is_tf else (len(states.shape) == 1)\n",
    "\n",
    "        if is_single_state:\n",
    "            states = states[None, ...]\n",
    "            actions = actions[None, ...]\n",
    "            next_states = next_states[None, ...]\n",
    "\n",
    "        scores = tf.zeros(actions.get_shape()[0]) if is_tf else np.zeros(actions.shape[0])\n",
    "\n",
    "        heading_penalty_factor = 10\n",
    "\n",
    "        # dont move front shin back so far that you tilt forward\n",
    "        front_leg = states[:, 6]\n",
    "        my_range = 0.2\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_leg >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_leg >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_shin = states[:, 7]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_shin >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_shin >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_foot = states[:, 8]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_foot >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_foot >= my_range) * heading_penalty_factor\n",
    "\n",
    "        scores -= (next_states[:, 16] - states[:, 16]) / 0.01\n",
    "\n",
    "        if is_single_state:\n",
    "            scores = scores[0]\n",
    "\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: id=1286687, shape=(6,), dtype=float32, numpy=\n",
       "array([ 0.88684106, -0.9599197 , -0.8087647 ,  0.93643665,  0.41107225,\n",
       "       -0.84757185], dtype=float32)>, state=(), info=())"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.policies import tf_py_policy\n",
    "from tf_agents.policies import py_tf_policy\n",
    "\n",
    "my_policy = ModelBasedPolicy(tf_env.time_step_spec(), tf_env.action_spec(), model)\n",
    "time_step = tf_env.reset()\n",
    "my_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class CollectionMode(Enum):\n",
    "    on_policy=0\n",
    "    off_policy=1\n",
    "\n",
    "class ModelBasedAgent(tf_agent.TFAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 time_step_spec,\n",
    "                 action_spec,\n",
    "                 dynamics_model_network,\n",
    "                 optimizer,\n",
    "                 debug_summaries=False,\n",
    "                 summarize_grads_and_vars=False,\n",
    "                 entropy_regularization=None,\n",
    "                 train_step_counter=None,\n",
    "                 name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "        self._dynamics_model_network = dynamics_model_network  \n",
    "        \n",
    "        policy = ModelBasedPolicy(\n",
    "                    time_step_spec=time_step_spec,\n",
    "                    action_spec=action_spec,\n",
    "                    dynamics_model_network=self._dynamics_model_network)\n",
    "        \n",
    "        collect_policy = policy\n",
    "\n",
    "        super(ModelBasedAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=2,\n",
    "            debug_summaries=debug_summaries,\n",
    "            summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "            train_step_counter=train_step_counter)\n",
    "        \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        \n",
    "        return time_steps, actions, next_time_steps\n",
    "                \n",
    "    def _train(self, experience, weights=None):\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "        state_diff = next_time_steps.observation - time_steps.observation\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_state_diff = self._dynamics_model_network(tf.concat([time_steps.observation, actions], 2), training=True)   \n",
    "            loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "            grads = tape.gradient(loss_value, self._dynamics_model_network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, self._dynamics_model_network.trainable_variables))\n",
    "            \n",
    "        return tf.nest.map_structure(tf.identity, tf_agent.LossInfo(loss_value, ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "input_size = env.action_spec().shape[0] + env.time_step_spec().observation.shape[0]\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "dynamics_model_network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "learning_rate = 1e-3  \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = ModelBasedAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    dynamics_model_network=dynamics_model_network,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000)\n",
    "\n",
    "\n",
    "initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "                    tf_env.time_step_spec(),\n",
    "                    tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: id=3532237, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=3532238, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=3532239, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=3532240, shape=(1, 17), dtype=float32, numpy=\n",
      "array([[ 3.2056827e-02,  2.9118804e-05, -7.2078496e-02, -5.7843365e-02,\n",
      "         9.9020109e-02, -9.7286545e-02,  2.1951772e-02,  5.6241442e-02,\n",
      "         1.3744840e-01,  1.6716242e-01, -1.8244857e-01,  2.3494018e-02,\n",
      "        -3.2553342e-03,  9.0839252e-02,  5.2028815e-03,  1.4341048e-02,\n",
      "         6.1431406e-03]], dtype=float32)>)\n",
      "Number of Steps:  1000\n",
      "Number of Episodes:  1\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "num_rollouts = 1\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, initial_collect_policy, observers, num_episodes=num_rollouts)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = collect_driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replay_buffer.gather_all().observation.shape)\n",
    "print(replay_buffer.get_next(num_steps=2)[0].observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelBasedAgent' object has no attribute 'LossInfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-eb2a93132820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/landuberdev/agents/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-bf3657a28910>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamics_model_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLossInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelBasedAgent' object has no attribute 'LossInfo'"
     ]
    }
   ],
   "source": [
    "time_step = None\n",
    "policy_state = initial_collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "num_iterations = 50\n",
    "\n",
    "for _ in range(1):\n",
    "    experience, _ = next(iterator)\n",
    "    loss_info = tf_agent.train(experience)\n",
    "    print(loss_info)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
