{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable v2 behavior\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n",
      "time_step_spec.observation BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation', environment.time_step_spec().observation)\n",
    "print('time_step_spec.reward', environment.time_step_spec().reward)\n",
    "print('time_step_spec.discount', environment.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', environment.time_step_spec().step_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01095514, -0.01147377,  0.01208805, -0.00147436], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "time_step = environment.reset()\n",
    "print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01072567,  0.18347275,  0.01205856, -0.290319  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01439512,  0.3784207 ,  0.00625218, -0.5791745 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.02196353,  0.5734545 , -0.00533131, -0.86988133], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03343263,  0.76864856, -0.02272893, -1.1642357 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04880559,  0.96405894, -0.04601365, -1.4639572 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.06808677,  1.1597134 , -0.07529279, -1.7706516 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.09128104,  1.3555999 , -0.11070582, -2.085764  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.11839304,  1.5516526 , -0.1524211 , -2.410524  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.14942609,  1.7477353 , -0.20063157, -2.7458766 ], dtype=float32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.1843808,  1.9436228, -0.2555491, -3.0924044], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(6,), dtype=dtype('float32'), name=None, minimum=[-1. -1. -1. -1. -1. -1.], maximum=[1. 1. 1. 1. 1. 1.])\n",
      "time_step_spec.observation BoundedArraySpec(shape=(17,), dtype=dtype('float32'), name=None, minimum=[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38], maximum=[3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation', env.time_step_spec().observation)\n",
    "print('time_step_spec.reward', env.time_step_spec().reward)\n",
    "print('time_step_spec.discount', env.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', env.time_step_spec().step_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(17,), dtype=tf.float32, name=None, minimum=array([-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38], dtype=float32), maximum=array([3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38], dtype=float32)))\n",
      "Action Specs: BoundedTensorSpec(shape=(6,), dtype=tf.float32, name=None, minimum=array([-1., -1., -1., -1., -1., -1.], dtype=float32), maximum=array([1., 1., 1., 1., 1., 1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.03723973,  0.08865002, -0.03914903, -0.08089941,  0.00092869,\n",
      "         0.08897705, -0.09163773,  0.04546549,  0.01857329,  0.09931803,\n",
      "        -0.03349365,  0.01707685, -0.04606488, -0.08050357, -0.110457  ,\n",
      "        -0.1112295 , -0.12238962]], dtype=float32)), array([[ 0.7629349 , -0.65688777,  0.8418231 , -0.6948137 ,  0.36854053,\n",
      "        -0.53573465]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([-0.17309183], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.02790391,  0.09771752,  0.18591745, -0.37610778,  0.3022899 ,\n",
      "        -0.20966941,  0.21534038, -0.11377001,  0.08316024, -0.03846378,\n",
      "         0.12309174,  5.5717487 , -6.138163  ,  7.2941446 , -7.6280513 ,\n",
      "         7.1246557 , -4.271552  ]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([-0.17309183], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.02790391,  0.09771752,  0.18591745, -0.37610778,  0.3022899 ,\n",
      "        -0.20966941,  0.21534038, -0.11377001,  0.08316024, -0.03846378,\n",
      "         0.12309174,  5.5717487 , -6.138163  ,  7.2941446 , -7.6280513 ,\n",
      "         7.1246557 , -4.271552  ]], dtype=float32)), array([[ 0.58457756,  0.30973625, -0.9974747 ,  0.7894635 ,  0.7630861 ,\n",
      "        -0.8224938 ]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.22893168], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-8.3713420e-02,  1.3361803e-02,  4.0475985e-01, -2.9172997e-03,\n",
      "        -2.0829742e-01, -1.7282575e-02,  2.9426774e-01, -4.4427574e-01,\n",
      "         7.2193998e-01, -1.5897454e+00, -2.2498026e+00,  4.1471019e+00,\n",
      "         1.1328389e+01, -1.4673685e+01,  8.8797083e+00, -9.8366201e-01,\n",
      "        -6.6639071e+00]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([0.22893168], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-8.3713420e-02,  1.3361803e-02,  4.0475985e-01, -2.9172997e-03,\n",
      "        -2.0829742e-01, -1.7282575e-02,  2.9426774e-01, -4.4427574e-01,\n",
      "         7.2193998e-01, -1.5897454e+00, -2.2498026e+00,  4.1471019e+00,\n",
      "         1.1328389e+01, -1.4673685e+01,  8.8797083e+00, -9.8366201e-01,\n",
      "        -6.6639071e+00]], dtype=float32)), array([[ 0.71965504, -0.5671654 ,  0.41330504, -0.23443317,  0.4346125 ,\n",
      "        -0.3035257 ]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.15280479], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-1.34898141e-01, -3.44732851e-02,  5.12708962e-01,\n",
      "         4.04546224e-03, -1.03250064e-01,  1.59972429e-01,\n",
      "         1.85234576e-01, -5.06955862e-01, -3.99099030e-02,\n",
      "        -5.79913914e-01, -5.63422561e-01,  1.91094398e-01,\n",
      "        -4.89745998e+00,  9.16913891e+00,  8.99850070e-01,\n",
      "        -2.75546670e+00,  1.32428157e+00]], dtype=float32))]\n",
      "Total reward: [0.20864464]\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "random_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                               time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "for i in range(num_steps):\n",
    "    action_step = random_policy.action(time_step)\n",
    "    action = action_step.action\n",
    "    next_time_step = tf_env.step(action)\n",
    "    transitions.append([time_step, action, next_time_step])\n",
    "    reward += next_time_step.reward\n",
    "    time_step = next_time_step\n",
    "    \n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.6215117]\n",
      " [0.1370089]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(0.6215117, shape=(), dtype=float32)\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "nested_array = tf.nest.flatten(tf.random.uniform(shape=[10, 299, 299, 3]).numpy());\n",
    "array = tf.nest.flatten(nested_array)\n",
    "len(array)\n",
    "arr = tf.random.uniform(shape=[2,1])\n",
    "print(arr)\n",
    "print(tf.math.reduce_max(arr))\n",
    "print(tf.math.argmax(arr, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Observation tf.Tensor(\n",
      "[[-0.03723973  0.08865002 -0.03914903 -0.08089941  0.00092869  0.08897705\n",
      "  -0.09163773  0.04546549  0.01857329  0.09931803 -0.03349365  0.01707685\n",
      "  -0.04606488 -0.08050357 -0.110457   -0.1112295  -0.12238962]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.7629349  -0.65688777  0.8418231  -0.6948137   0.36854053 -0.53573465]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[-0.02790391  0.09771752  0.18591745 -0.37610778  0.3022899  -0.20966941\n",
      "   0.21534038 -0.11377001  0.08316024 -0.03846378  0.12309174  5.5717487\n",
      "  -6.138163    7.2941446  -7.6280513   7.1246557  -4.271552  ]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.58457756  0.30973625 -0.9974747   0.7894635   0.7630861  -0.8224938 ]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[-8.3713420e-02  1.3361803e-02  4.0475985e-01 -2.9172997e-03\n",
      "  -2.0829742e-01 -1.7282575e-02  2.9426774e-01 -4.4427574e-01\n",
      "   7.2193998e-01 -1.5897454e+00 -2.2498026e+00  4.1471019e+00\n",
      "   1.1328389e+01 -1.4673685e+01  8.8797083e+00 -9.8366201e-01\n",
      "  -6.6639071e+00]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.71965504 -0.5671654   0.41330504 -0.23443317  0.4346125  -0.3035257 ]], shape=(1, 6), dtype=float32)\n",
      "[array([14.457764], dtype=float32), array([77.31396], dtype=float32), array([61.546093], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input_size = tf_env.action_spec().shape[0] + tf_env.time_step_spec().observation.shape[0]\n",
    "print(input_size)\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_history = []\n",
    "\n",
    "for time_step, action, next_time_step in transitions:\n",
    "    state_diff = next_time_step.observation - time_step.observation\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        print('Observation', time_step.observation)\n",
    "        print('Action', action)\n",
    "        pred_state_diff = model(tf.concat([time_step.observation, action], 1), training=True)   \n",
    "        loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "    \n",
    "    loss_history.append(loss_value.numpy())\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "\n",
    "print(loss_history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "\n",
    "class ModelBasedPolicy(tf_policy.Base):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 time_step_spec, \n",
    "                 action_spec, \n",
    "                 dynamics_model_network,\n",
    "                 num_random_action_selection=4096,\n",
    "                 horizon=15):\n",
    "        \n",
    "        if time_step_spec is None:\n",
    "            time_step_spec = ts.time_step_spec()\n",
    "        self._dynamics_model_network = dynamics_model_network\n",
    "        self._num_random_action_selection=num_random_action_selection\n",
    "        self._horizon = horizon\n",
    "        \n",
    "        super(ModelBasedPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec, action_spec=action_spec)\n",
    "        \n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        \n",
    "        random_actions = tensor_spec.sample_spec_nest(\n",
    "            self._action_spec, seed, outer_dims=[self._num_random_action_selection, self._horizon])\n",
    "        start_state = tf.tile(time_step.observation, [self._num_random_action_selection, 1])\n",
    "        cost = tf.zeros(shape=[self._num_random_action_selection, 1])\n",
    "        curr_state = start_state\n",
    "        for h in range(self._horizon):\n",
    "            next_state = curr_state + self._dynamics_model_network(tf.concat([curr_state, random_actions[:, h, :]], 1))\n",
    "            cost += self._cost_fn(curr_state, tf.constant(random_actions[:, h, :]), next_state)\n",
    "            curr_state = next_state\n",
    "           \n",
    "        min_cost_index = tf.math.argmin(cost, axis=1)[0]\n",
    "        return policy_step.PolicyStep(tf.expand_dims(random_actions[min_cost_index, 0, :], 0), policy_state)\n",
    "    \n",
    "    def _cost_fn(self, states, actions, next_states):\n",
    "        is_tf = tf.is_tensor(states)\n",
    "        is_single_state = (len(states.get_shape()) == 1) if is_tf else (len(states.shape) == 1)\n",
    "\n",
    "        if is_single_state:\n",
    "            states = states[None, ...]\n",
    "            actions = actions[None, ...]\n",
    "            next_states = next_states[None, ...]\n",
    "\n",
    "        scores = tf.zeros(actions.get_shape()[0]) if is_tf else np.zeros(actions.shape[0])\n",
    "\n",
    "        heading_penalty_factor = 10\n",
    "\n",
    "        # dont move front shin back so far that you tilt forward\n",
    "        front_leg = states[:, 6]\n",
    "        my_range = 0.2\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_leg >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_leg >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_shin = states[:, 7]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_shin >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_shin >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_foot = states[:, 8]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_foot >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_foot >= my_range) * heading_penalty_factor\n",
    "\n",
    "        scores -= (next_states[:, 16] - states[:, 16]) / 0.01\n",
    "\n",
    "        if is_single_state:\n",
    "            scores = scores[0]\n",
    "\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyStep(action=<tf.Tensor: id=16795473, shape=(1, 6), dtype=float32, numpy=\n",
      "array([[-0.98933554, -0.43751025, -0.74255943,  0.7513337 , -0.1552701 ,\n",
      "         0.25850296]], dtype=float32)>, state=(), info=())\n",
      "PolicyStep(action=<tf.Tensor: id=16795492, shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1007166e-01, -4.9070120e-02, -2.3317337e-04, -5.4304695e-01,\n",
      "        -4.5401669e-01,  6.7937756e-01]], dtype=float32)>, state=(), info=())\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import tf_py_policy\n",
    "from tf_agents.policies import py_tf_policy\n",
    "\n",
    "my_policy = ModelBasedPolicy(tf_env.time_step_spec(), tf_env.action_spec(), model)\n",
    "my_policy2 = random_tf_policy.RandomTFPolicy(\n",
    "                    tf_env.time_step_spec(),\n",
    "                    tf_env.action_spec())\n",
    "time_step = tf_env.reset()\n",
    "action_step = my_policy.action(time_step)\n",
    "action_step2 = my_policy2.action(time_step)\n",
    "print(action_step)\n",
    "print(action_step2)\n",
    "#tf_env.step(action_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "\n",
    "class ModelBasedAgent(tf_agent.TFAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 time_step_spec,\n",
    "                 action_spec,\n",
    "                 dynamics_model_network,\n",
    "                 optimizer,\n",
    "                 debug_summaries=False,\n",
    "                 summarize_grads_and_vars=False,\n",
    "                 entropy_regularization=None,\n",
    "                 train_step_counter=None,\n",
    "                 name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "        self._dynamics_model_network = dynamics_model_network  \n",
    "        \n",
    "        policy = ModelBasedPolicy(\n",
    "                    time_step_spec=time_step_spec,\n",
    "                    action_spec=action_spec,\n",
    "                    dynamics_model_network=self._dynamics_model_network)\n",
    "        \n",
    "        collect_policy = policy\n",
    "\n",
    "        super(ModelBasedAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=2,\n",
    "            debug_summaries=debug_summaries,\n",
    "            summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "            train_step_counter=train_step_counter)\n",
    "        \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        \n",
    "        return time_steps, actions, next_time_steps\n",
    "                \n",
    "    def _train(self, experience, weights=None):\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "        state_diff = next_time_steps.observation - time_steps.observation\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_state_diff = self._dynamics_model_network(tf.concat([time_steps.observation, actions], 2), training=True)   \n",
    "            loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "            grads = tape.gradient(loss_value, self._dynamics_model_network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, self._dynamics_model_network.trainable_variables))\n",
    "            \n",
    "        return tf.nest.map_structure(tf.identity, tf_agent.LossInfo(loss_value, ()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "input_size = env.action_spec().shape[0] + env.time_step_spec().observation.shape[0]\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "dynamics_model_network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "learning_rate = 1e-3  \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = ModelBasedAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    dynamics_model_network=dynamics_model_network,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000)\n",
    "\n",
    "\n",
    "initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "                    tf_env.time_step_spec(),\n",
    "                    tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "num_rollouts = 1\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, initial_collect_policy, observers, num_episodes=num_rollouts)\n",
    "onpolicy_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, agent.collect_policy, observers, num_episodes=1)\n",
    "\n",
    "#print('final_time_step', final_time_step)\n",
    "#print('Number of Steps: ', env_steps.result().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Number of Episodes:  1\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "time_step = None\n",
    "policy_state = initial_collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "initial_collect_driver.run()\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "model_iterations = 1\n",
    "policy_iterations = 1\n",
    "\n",
    "for _ in range(model_iterations):\n",
    "    print('Hello')\n",
    "    experience, _ = next(iterator)\n",
    "    loss_info = agent.train(experience)\n",
    "    \n",
    "print('Number of Episodes: ', num_episodes.result().numpy()) \n",
    "for _ in range(policy_iterations):\n",
    "    onpolicy_collect_driver.run()\n",
    "    experience, _ = next(iterator)\n",
    "    loss_info = agent.train(experience)\n",
    "   \n",
    "\n",
    "print('Done training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video);\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0523 17:00:06.227456 140425894250240 _io.py:361] IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3\n",
    "video_filename = 'imageio.mp4'\n",
    "with imageio.get_writer(video_filename, fps=60) as video:\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = tf_env.reset()\n",
    "        video.append_data(env.render())\n",
    "        while not time_step.is_last():\n",
    "            action_step = agent.policy.action(time_step)\n",
    "            time_step = tf_env.step(action_step.action)\n",
    "            video.append_data(env.render())\n",
    "        \n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
