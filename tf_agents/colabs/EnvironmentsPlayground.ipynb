{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable v2 behavior\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n",
      "time_step_spec.observation BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation', environment.time_step_spec().observation)\n",
    "print('time_step_spec.reward', environment.time_step_spec().reward)\n",
    "print('time_step_spec.discount', environment.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', environment.time_step_spec().step_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.03054085,  0.02499142,  0.0277015 ,  0.01631059], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "time_step = environment.reset()\n",
    "print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.03004102,  0.21970537,  0.02802771, -0.26750508], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.02564692,  0.41441634,  0.02267761, -0.55121773], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.01735859,  0.6092126 ,  0.01165325, -0.83667034], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.00517434,  0.8041734 , -0.00508015, -1.1256658 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01090913,  0.9993616 , -0.02759347, -1.4199378 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03089636,  1.194814  , -0.05599222, -1.7211162 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.05479264,  1.3905306 , -0.09041455, -2.0306842 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.08260325,  1.586462  , -0.13102823, -2.3499267 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.1143325 ,  1.782493  , -0.17802677, -2.6798646 ], dtype=float32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.14998235,  1.9784241 , -0.23162405, -3.0211802 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(6,), dtype=dtype('float32'), name=None, minimum=[-1. -1. -1. -1. -1. -1.], maximum=[1. 1. 1. 1. 1. 1.])\n",
      "time_step_spec.observation BoundedArraySpec(shape=(17,), dtype=dtype('float32'), name=None, minimum=[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      " -3.4028235e+38], maximum=[3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38\n",
      " 3.4028235e+38 3.4028235e+38])\n",
      "time_step_spec.reward ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "time_step_spec.discount BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.step_type ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation', env.time_step_spec().observation)\n",
    "print('time_step_spec.reward', env.time_step_spec().reward)\n",
    "print('time_step_spec.discount', env.time_step_spec().discount)\n",
    "print('time_step_spec.step_type', env.time_step_spec().step_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(17,), dtype=tf.float32, name=None, minimum=array([-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38, -3.4028235e+38, -3.4028235e+38, -3.4028235e+38,\n",
      "       -3.4028235e+38], dtype=float32), maximum=array([3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38, 3.4028235e+38, 3.4028235e+38, 3.4028235e+38,\n",
      "       3.4028235e+38], dtype=float32)))\n",
      "Action Specs: BoundedTensorSpec(shape=(6,), dtype=tf.float32, name=None, minimum=array([-1., -1., -1., -1., -1., -1.], dtype=float32), maximum=array([1., 1., 1., 1., 1., 1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.04096488, -0.04456011,  0.03539135,  0.00959783,  0.0405442 ,\n",
      "        -0.0952404 ,  0.05411316,  0.06147752,  0.12355213, -0.21735004,\n",
      "         0.11308104,  0.07886522, -0.23104899, -0.05057145, -0.06557063,\n",
      "        -0.07195956,  0.05116129]], dtype=float32)), array([[ 0.12916636, -0.4701333 , -0.1957438 , -0.2429452 ,  0.41729975,\n",
      "        -0.45938134]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.03592229], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02386188, -0.02302459,  0.08942886, -0.18066144, -0.06220927,\n",
      "        -0.15312383,  0.17080186, -0.10673446,  0.08011492, -0.5249965 ,\n",
      "         0.48879454,  0.95780903, -4.3135786 , -2.4086382 , -1.3559316 ,\n",
      "         2.829659  , -4.6082335 ]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([0.03592229], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02386188, -0.02302459,  0.08942886, -0.18066144, -0.06220927,\n",
      "        -0.15312383,  0.17080186, -0.10673446,  0.08011492, -0.5249965 ,\n",
      "         0.48879454,  0.95780903, -4.3135786 , -2.4086382 , -1.3559316 ,\n",
      "         2.829659  , -4.6082335 ]], dtype=float32)), array([[-0.08561516, -0.4688561 , -0.9777341 , -0.9109969 ,  0.04478097,\n",
      "        -0.3444512 ]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([-0.59794515], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-2.4857866e-02,  1.9296164e-02,  5.4148696e-03, -2.8643462e-01,\n",
      "        -4.2511880e-01, -4.0445280e-01,  2.0377558e-01, -2.3186734e-01,\n",
      "        -5.5300891e-01, -1.2891542e+00,  1.0162114e+00, -2.9272718e+00,\n",
      "        -8.3611608e-01, -7.5777555e+00, -6.2750554e+00, -1.1666615e+00,\n",
      "        -1.0718255e+00]], dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([-0.59794515], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-2.4857866e-02,  1.9296164e-02,  5.4148696e-03, -2.8643462e-01,\n",
      "        -4.2511880e-01, -4.0445280e-01,  2.0377558e-01, -2.3186734e-01,\n",
      "        -5.5300891e-01, -1.2891542e+00,  1.0162114e+00, -2.9272718e+00,\n",
      "        -8.3611608e-01, -7.5777555e+00, -6.2750554e+00, -1.1666615e+00,\n",
      "        -1.0718255e+00]], dtype=float32)), array([[ 0.6237068 ,  0.66364694, -0.86738825,  0.01324368,  0.04736805,\n",
      "        -0.6268742 ]], dtype=float32), TimeStep(step_type=array([1], dtype=int32), reward=array([0.17918764], dtype=float32), discount=array([1.], dtype=float32), observation=array([[-0.11721179, -0.01196623,  0.03388531,  0.11385418, -0.41638413,\n",
      "        -0.28652024, -0.03101892, -0.32529125,  0.8402051 , -2.109448  ,\n",
      "        -1.4211122 ,  4.044221  ,  9.908107  ,  0.35223132,  6.1244793 ,\n",
      "        -4.8410172 , -2.0509224 ]], dtype=float32))]\n",
      "Total reward: [-0.3828352]\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "random_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                               time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "for i in range(num_steps):\n",
    "    action_step = random_policy.action(time_step)\n",
    "    action = action_step.action\n",
    "    next_time_step = tf_env.step(action)\n",
    "    transitions.append([time_step, action, next_time_step])\n",
    "    reward += next_time_step.reward\n",
    "    time_step = next_time_step\n",
    "    \n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.28794646]\n",
      " [0.711594  ]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(0.711594, shape=(), dtype=float32)\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "nested_array = tf.nest.flatten(tf.random.uniform(shape=[10, 299, 299, 3]).numpy());\n",
    "array = tf.nest.flatten(nested_array)\n",
    "len(array)\n",
    "arr = tf.random.uniform(shape=[2,1])\n",
    "print(arr)\n",
    "print(tf.math.reduce_max(arr))\n",
    "print(tf.math.argmax(arr, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Observation tf.Tensor(\n",
      "[[ 0.04096488 -0.04456011  0.03539135  0.00959783  0.0405442  -0.0952404\n",
      "   0.05411316  0.06147752  0.12355213 -0.21735004  0.11308104  0.07886522\n",
      "  -0.23104899 -0.05057145 -0.06557063 -0.07195956  0.05116129]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.12916636 -0.4701333  -0.1957438  -0.2429452   0.41729975 -0.45938134]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[ 0.02386188 -0.02302459  0.08942886 -0.18066144 -0.06220927 -0.15312383\n",
      "   0.17080186 -0.10673446  0.08011492 -0.5249965   0.48879454  0.95780903\n",
      "  -4.3135786  -2.4086382  -1.3559316   2.829659   -4.6082335 ]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[-0.08561516 -0.4688561  -0.9777341  -0.9109969   0.04478097 -0.3444512 ]], shape=(1, 6), dtype=float32)\n",
      "Observation tf.Tensor(\n",
      "[[-2.4857866e-02  1.9296164e-02  5.4148696e-03 -2.8643462e-01\n",
      "  -4.2511880e-01 -4.0445280e-01  2.0377558e-01 -2.3186734e-01\n",
      "  -5.5300891e-01 -1.2891542e+00  1.0162114e+00 -2.9272718e+00\n",
      "  -8.3611608e-01 -7.5777555e+00 -6.2750554e+00 -1.1666615e+00\n",
      "  -1.0718255e+00]], shape=(1, 17), dtype=float32)\n",
      "Action tf.Tensor([[ 0.6237068   0.66364694 -0.86738825  0.01324368  0.04736805 -0.6268742 ]], shape=(1, 6), dtype=float32)\n",
      "[array([3.3094654], dtype=float32), array([6.439027], dtype=float32), array([24.555866], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input_size = env.action_spec().shape[0] + env.time_step_spec().observation.shape[0]\n",
    "print(input_size)\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_history = []\n",
    "\n",
    "for time_step, action, next_time_step in transitions:\n",
    "    state_diff = next_time_step.observation - time_step.observation\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        print('Observation', time_step.observation)\n",
    "        print('Action', action)\n",
    "        pred_state_diff = model(tf.concat([time_step.observation, action], 1), training=True)   \n",
    "        loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "    \n",
    "    loss_history.append(loss_value.numpy())\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "\n",
    "print(loss_history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "\n",
    "class ModelBasedPolicy(py_policy.Base):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 time_step_spec, \n",
    "                 action_spec, \n",
    "                 dynamics_model_network,\n",
    "                 num_random_action_selection=4096,\n",
    "                 horizon=15):\n",
    "        \n",
    "        if time_step_spec is None:\n",
    "            time_step_spec = ts.time_step_spec()\n",
    "        self._dynamics_model_network = dynamics_model_network\n",
    "        self._num_random_action_selection=num_random_action_selection\n",
    "        self._horizon = horizon\n",
    "        \n",
    "        super(ModelBasedPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec, action_spec=action_spec)\n",
    "        \n",
    "    def _action(self, time_step, policy_state):\n",
    "        \n",
    "        random_actions = array_spec.sample_spec_nest(\n",
    "            self._action_spec, np.random.RandomState(), outer_dims=[self._num_random_action_selection, self._horizon])\n",
    "        start_state = tf.tile(time_step.observation, [self._num_random_action_selection, 1])\n",
    "        cost = tf.zeros(shape=[self._num_random_action_selection, 1])\n",
    "        curr_state = start_state\n",
    "        for h in range(self._horizon):\n",
    "            next_state = curr_state + self._dynamics_model_network(tf.concat([curr_state, random_actions[:, h, :]], 1))\n",
    "            cost += self._cost_fn(curr_state, tf.constant(random_actions[:, h, :]), next_state)\n",
    "            curr_state = next_state\n",
    "           \n",
    "        min_cost_index = tf.math.argmin(cost, axis=1)[0]\n",
    "        return policy_step.PolicyStep(random_actions[min_cost_index, 0, :], policy_state)\n",
    "    \n",
    "    def _cost_fn(self, states, actions, next_states):\n",
    "        is_tf = tf.is_tensor(states)\n",
    "        is_single_state = (len(states.get_shape()) == 1) if is_tf else (len(states.shape) == 1)\n",
    "\n",
    "        if is_single_state:\n",
    "            states = states[None, ...]\n",
    "            actions = actions[None, ...]\n",
    "            next_states = next_states[None, ...]\n",
    "\n",
    "        scores = tf.zeros(actions.get_shape()[0]) if is_tf else np.zeros(actions.shape[0])\n",
    "\n",
    "        heading_penalty_factor = 10\n",
    "\n",
    "        # dont move front shin back so far that you tilt forward\n",
    "        front_leg = states[:, 6]\n",
    "        my_range = 0.2\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_leg >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_leg >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_shin = states[:, 7]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_shin >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_shin >= my_range) * heading_penalty_factor\n",
    "\n",
    "        front_foot = states[:, 8]\n",
    "        my_range = 0\n",
    "        if is_tf:\n",
    "            scores += tf.cast(front_foot >= my_range, tf.float32) * heading_penalty_factor\n",
    "        else:\n",
    "            scores += (front_foot >= my_range) * heading_penalty_factor\n",
    "\n",
    "        scores -= (next_states[:, 16] - states[:, 16]) / 0.01\n",
    "\n",
    "        if is_single_state:\n",
    "            scores = scores[0]\n",
    "\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=array([ 0.12087511,  0.8673162 ,  0.6929906 , -0.83383524,  0.84574324,\n",
       "        0.8653695 ], dtype=float32), state=(), info=())"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_policy = ModelBasedPolicy(env.time_step_spec(), env.action_spec(), model)\n",
    "time_step = tf_env.reset()\n",
    "my_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class CollectionMode(Enum):\n",
    "    on_policy=0\n",
    "    off_policy=1\n",
    "\n",
    "class ModelBasedAgent(tf_agent.TFAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 time_step_spec,\n",
    "                 action_spec,\n",
    "                 dynamics_model_network,\n",
    "                 optimizer,\n",
    "                 debug_summaries=False,\n",
    "                 summarize_grads_and_vars=False,\n",
    "                 entropy_regularization=None,\n",
    "                 train_step_counter=None,\n",
    "                 name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "        self._dynamics_model_network = dynamics_model_network    \n",
    "        collect_policy = random_py_policy.RandomPyPolicy(\n",
    "                    time_step_spec=time_step_spec,\n",
    "                    action_spec=action_spec)\n",
    "        policy = greedy_policy.GreedyPolicy(collect_policy)\n",
    "\n",
    "        super(ModelBasedAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=None,\n",
    "            debug_summaries=debug_summaries,\n",
    "            summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "            train_step_counter=train_step_counter)\n",
    "        \n",
    "    \n",
    "    def set_collection_policy(self, collection_mode):\n",
    "                # Type checking\n",
    "        if not isinstance(collection_mode, CollectionMode):\n",
    "            raise TypeError('collection_mode must be an instance of CollectionMode Enum')\n",
    "        if collection_mode is CollectionMode.on_policy:\n",
    "                self._collect_policy = ModelBasedPolicy(\n",
    "                    time_step_spec=super().time_step_spec,\n",
    "                    action_spec=super().action_spec,\n",
    "                    dynamics_model_network=self._dynamics_model_network)\n",
    "        else:\n",
    "                \n",
    "                self._collect_policy = random_py_policy.RandomPyPolicy(\n",
    "                    time_step_spec=super().time_step_spec(),\n",
    "                    action_spec=super().action_spec())\n",
    "                \n",
    "    def _train(self, experience, weights=None):\n",
    "        state_diff = experience.next_time_step.observation - experience.time_step.observation\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_state_diff = self._dynamics_model_network(tf.concat([experience.time_step.observation, experience.action], 1), training=True)   \n",
    "            loss_value = tf.losses.mean_squared_error(state_diff, pred_state_diff)\n",
    "            grads = tape.gradient(loss_value, self._dynamics_model_network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, self._dynamics_model_network.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_mujoco.load('HalfCheetah-v2')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "input_size = env.action_spec().shape[0] + env.time_step_spec().observation.shape[0]\n",
    "output_size = env.time_step_spec().observation.shape[0]\n",
    "dynamics_model_network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(input_size, )),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "learning_rate = 1e-3  \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = ModelBasedAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    dynamics_model_network=dynamics_model_network,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec =  (env.time_step_spec(), env.action_spec(), env.time_step_spec())\n",
    "\n",
    "batch_size = 1\n",
    "max_length = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
